

# RAG 초간단 핸즈온



# 1. 개요



**“RAG가 왜 필요한지 몸으로 이해하는” 초간단 핸즈온**

* LLM이 모르는 정보(내 문서)를 넣어 그 문서를 근거로 답하게 만든다

* 이게 **RAG의 전부**



RAG를 한 문장으로 하면

* **RAG = 검색(Retrieval) + 생성(Generation)**

* → “문서를 찾아서 → 그걸 보고 답하게 함”





# 2. 구조



```
질문
 ↓
(문서 검색)
 ↓
문서 + 질문
 ↓
LLM
 ↓
답변
```





# 3. 환경셋팅



## 1) Container 환경 구축

```sh
$ docker ps

$ docker run -d --name python python sleep 365d

$ docker exec -it python bash
```



## 2) pip install

```
pip install langchain langchain-openai
```

> 벡터DB, 임베딩, FAISS  는 불필요한 간단한 RAG 테스트





## 3) api key 연결

```sh

export OPENAI_API_KEY="sk-proj-9ruwQXScirELhs_yXKb...."

```







# 4. LLM만 쓰면 틀리는 예제



```sh

$ cat > main11.py

```



```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

print(llm.invoke("cloud-ai-lab은 뭐야?").content)
```



```sh

$ python main11.py 
Cloud AI Lab는 클라우드 기반의 인공지능 연구실을 의미합니다. 이는 클라우드 환경에서 인공지능 및 머신 러닝 모델을 개발, 훈련, 배포하는데 사용되는 연구실을 말합니다. 클라우드를 통해 인프라를 관리하고 데이터에 쉽게 접근할 수 있어 효율적으로 인공지능 모델을 구축하고 훈련할 수 있습니다.

```



👉 **엉뚱한 답**이 나온다.

(LLM은 cloud-ai-lab을 모름)





# 5. RAG용 “내 문서” 준비



```
docs = """
cloud-ai-lab은 Cloud사업담당 팀장들이
LLM, RAG, Agent, MCP를 학습하기 위해 만든 스터디 저장소이다.
"""

```

이 문서가 **유일한 진실(Source of Truth)**



# 6. RAG 핵심 코드



```sh

cat > main12.py

```



```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-3.5-turbo")

docs = """
cloud-ai-lab은 Cloud사업담당 팀장들이
LLM, RAG, Agent, MCP를 학습하기 위해 만든 스터디 저장소이다.
"""

prompt = PromptTemplate(
    input_variables=["question", "context"],
    template="""
    아래 문서를 참고해서 질문에 답해라.
    문서에 없는 내용은 모른다고 말해라.

    문서:
    {context}

    질문:
    {question}
    """
)

rag_chain = (
    prompt
    | llm
    | StrOutputParser()
)

result = rag_chain.invoke({
    "question": "cloud-ai-lab은 뭐야?",
    "context": docs
})

print(result)
```



```sh

$ python main12.py
cloud-ai-lab은 Cloud사업담당 팀장들이 LLM, RAG, Agent, MCP를 학습하기 위해 만든 스터디 저장소이다.

```





# 7. 핵심 포인트



## 핵심 차이

| **방식** | **특징**      |
| -------- | ------------- |
| LLM 단독 | **추측**      |
| RAG      | **문서 근거** |

LLM이:

- ❌ 상상해서 말하지 못함
- ⭕ 문서에 있는 내용만 사용



# 8. 일부러 깨뜨려 보기

아래처럼 문서에 없는 내용 질의 하기.

```
result = rag_chain.invoke({
    "question": "cloud-ai-lab은 언제 상장했어?",
    "context": docs
})
print(result)
```



```sh

cat > main13.py

```





```python

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-3.5-turbo")

docs = """
cloud-ai-lab은 Cloud사업담당 팀장들이
LLM, RAG, Agent, MCP를 학습하기 위해 만든 스터디 저장소이다.
"""

prompt = PromptTemplate(
    input_variables=["question", "context"],
    template="""
    아래 문서를 참고해서 질문에 답해라.
    문서에 없는 내용은 모른다고 말해라.

    문서:
    {context}

    질문:
    {question}
    """
)

rag_chain = (
    prompt
    | llm
    | StrOutputParser()
)

result = rag_chain.invoke({
    "question": "cloud-ai-lab은 언제 상장했어?",
    "context": docs
})

print(result)

```



```sh

$ python main13.py

위 문서에는 cloud-ai-lab이 언제 상장했는지에 대한 정보가 포함되어 있지 않습니다. 
따라서 해당 정보에 대해서는 알 수 없습니다.

```





👉 정상적인 RAG라면:

```
문서에 해당 내용이 없습니다.
```

이게 **RAG의 신뢰성 포인트**





# 9. 꼭 기억할 3가지



## ① RAG는 기술이 아니라 패턴

- 검색 → 문서 → 프롬프트



## ② 벡터DB는 옵션

- 개념 이해에는 불필요



## ③ 프롬프트가 핵심

```
문서에 없는 건 말하지 마라
```





# 10. 한 줄 요약 (스터디용)



> **RAG는 “LLM에게 기억을 주입하는 기술”이 아니라**

> **“LLM에게 참고자료를 들고 오게 하는 구조”다.**

